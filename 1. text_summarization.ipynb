{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# 1.Text Summarization with Generative Models on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points. In this notebook, you go through a few examples of how large language models can help with generating summaries based on text.\n",
    "\n",
    "Learn more about text summarization in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you will learn how to use generative models to summarize information from text by working through the following examples:\n",
    "- Transcript summarization\n",
    "- Summarizing text into bullet points\n",
    "- Dialogue summarization with to-dos\n",
    "- Hashtag tokenization\n",
    "- Title & heading generation\n",
    "\n",
    "You also learn how to evaluate model-generated summaries by comparing to human-created summaries using ROUGE as an evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bs9TZo0GJKCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "148dd6321946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.10/site-packages (1.27.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Obtaining dependency information for google-cloud-aiplatform from https://files.pythonhosted.org/packages/0b/7c/fcbb346dbfe4de0b4cccf33ed114b94921a970f7624a8b42c1aa54b7699d/google_cloud_aiplatform-1.31.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_cloud_aiplatform-1.31.1-py2.py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.3)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.15)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n",
      "Using cached google_cloud_aiplatform-1.31.1-py2.py3-none-any.whl (2.8 MB)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.27.0\n",
      "    Uninstalling google-cloud-aiplatform-1.27.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.27.0\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.31.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Let's start by importing the libraries that we will need for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Import models\n",
    "\n",
    "Here we load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vertexai.language_models._language_models.TextGenerationModel object at 0x7fc6b444dcc0>\n"
     ]
    }
   ],
   "source": [
    "print(generation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu1UAhoTKn51"
   },
   "source": [
    "## Section 1. Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. Fundamental Transcript summarization\n",
    "\n",
    "In this first example, you summarize a piece of article on Google Next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GCN23_GE_BlogHeader_2436x1200_.max-1200x416.format-webp_vlvbanX.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgZvJeBpJKCS"
   },
   "source": [
    "In my conversations with business leaders over the past few years, I’ve heard a similar theme: They want a partner that's been on the cutting edge of technology breakthroughs, be it from desktop to mobile, to the cloud, or now, to AI. And a partner who can help navigate and lead through them.\n",
    "\n",
    "These shifts can be really exciting — and they can also bring uncertainty. That’s definitely true of the shift to AI. It will be the most profound shift we’ll see in our lifetimes. It will touch every sector, every industry, every business function. And significantly change the way we live and work.\n",
    "\n",
    "This isn’t just the future. We’re already starting to experience the benefits right now.\n",
    "\n",
    "As a company, we’ve been preparing for this moment for some time. And for the last seven years, we’ve taken an AI-first approach, applying AI to make our products radically more helpful.\n",
    "\n",
    "We believe that making AI helpful for everyone is the most important way we'll deliver on our mission in the next decade. That’s why we’ve invested in the very best tooling, foundation models and infrastructure, across both TPUs and GPUs.\n",
    "\n",
    "These underlying technologies are helping us transform our products and businesses —and they’ll help you transform yours.\n",
    "\n",
    "Let’s take Search. We’ve spent the last 25 years trying to perfect Google Search — and we’re still not done. Today, we’re using generative AI to reimagine the experience and take more of the work out of searching. We call this the Search Generative Experience, or SGE for short. SGE uses advanced AI to help you get the gist of a topic quickly with an overview, easily follow up on questions in a conversational way, or even make coding tasks easier. We’ve had really positive feedback from people who’ve used it so far, helping us learn and improve fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Let's create a summary in 5 sentences of this content: \n",
    "\n",
    "<ADD YOUR CONTENTS> \n",
    "\n",
    "Summary: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. TL;DR Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aade04b2e86a"
   },
   "source": [
    "Instead of a summary, we can ask for a TL;DR (\"too long; didn't read\"). You can compare the differences between the outputs generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Let's create a TL;DR for the below content: \n",
    "\n",
    "<ADD YOUR CONTENTS> \n",
    "\n",
    "Summary: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any difference on results? \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Share your thoughts: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PATmHivJKCS"
   },
   "source": [
    "## Section 2. Summarize text into bullet points\n",
    "In the following example, you use same text on Google Cloud Next, but ask the model to summarize it in bullet-point form. Feel free to change the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2orkDF2VJKCT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Google Cloud Next is an annual conference for cloud computing professionals.\n",
      "* The conference features keynote addresses from Google executives, as well as sessions from Google engineers and partners.\n",
      "* Topics covered at the conference include machine learning, artificial intelligence, big data, and cloud computing.\n",
      "* The conference is typically held in San Francisco, California.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary in four bullet points. \n",
    "\n",
    "<COPY THE GOOGLE CLOUD NEXT ARTICLE FROM ABOVE SECTION 1 EXAMPLE>\n",
    "\n",
    "Bulletpoints:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=256, top_k=1, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE7y-clBJKCT"
   },
   "source": [
    "## Section 3. Dialogue summarization with to-dos\n",
    "\n",
    "Imagine you as a support representative, having a support chat with a customer. You have the support chat dialogue as below and want to use *prompt* to get better insights. How would you create your prompt? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](https://img.freepik.com/free-photo/support-community-aid-help-team-assistance-concept_53876-123806.jpg?w=1800&t=st=1693334874~exp=1693335474~hmac=f4bf09aa03183925fafe1a584c6098af419e50415455018754a9f638a231eaf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Dialogue Example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Chat started: 2020-01-02 11:51 PM UTC\n",
    "\n",
    "(11:51:58 PM) Jacob: Hello Support\n",
    "(11:52:02 PM) *** Mandy joined the chat ***\n",
    "(11:52:13 PM) Mandy: Hey Jacob! How's the new year treating you so far?\n",
    "(11:52:51 PM) Jacob: Pretty good thank you. I have a small issue which I hope is a quick fix re: this explore: lookerurl.. \n",
    "(11:54:13 PM) Jacob: In the Explore drope down there's double listings for: test_results_co2 (aliased to Results) and test_alarm_log_rt (aliased to Alarm Log)\n",
    "(11:54:18 PM) Jacob: drop down*\n",
    "(11:54:50 PM) Jacob: How can I update my lookml to only show a single entry in the Explore drop down for these two?\n",
    "(11:55:35 PM) Mandy: Ah, let me take a moment to log in\n",
    "(11:59:28 PM) Mandy: It looks like the Alarm Log, Results entries are coming from both models - test_historical, test_real_time\n",
    "(12:01:12 AM) Mandy: So it looks like one of the reasons why we see duplicate explores in the Explore menu is if we include a model file in a view file\n",
    "(12:01:36 AM) Jacob: lookerurl\n",
    "(12:01:53 AM) Jacob: Hmm, my only include is: include: \"/views/**/*.view\"\n",
    "(12:01:58 AM) Jacob: What am I missing?\n",
    "(12:03:35 AM) Mandy: Ah, so we see this when we include a model file in a NDT view file and then when another model is added which includes the NDT view file, all the explores from the original model file shows up in both models\n",
    "(12:03:44 AM) Mandy: So for example, this NDT here: lookerurl\n",
    "(12:04:07 AM) Mandy: One way around this is to use a separate explore file, or another way is to not include the NDT view file in the new model file\n",
    "(12:05:06 AM) Mandy: With include: \"/views/**/*.view\", we're including all view files in the model file\n",
    "(12:05:47 AM) Jacob: Ahh, so its because I'm using a derived table based on the other explore? It looks like the model include is required in the PDT\n",
    "(12:06:34 AM) Jacob: Oh, I see- I just need to not include that PDT in the other model\n",
    "(12:06:56 AM) Mandy: Right, so we won't include that view in that model\n",
    "(12:07:51 AM) Jacob: Ok that makes sense. I'll organize my views into folders and update the includes\n",
    "(12:07:58 AM) Jacob: thank you for your help Mandy!\n",
    "(12:08:10 AM) Mandy: Anytime, Jacob! Any other questions for now?\n",
    "(12:08:25 AM) Jacob: Nope, I\"m good. Happy New Year!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV-BWzRhJKCT"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Please generate a summary of the following conversation and at the end summarize the to-do's for the support Agent:\n",
    "\n",
    "<ADD SUPPORT DIALOGUE HERE>\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out your own prompts! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Do you have any other prompt you want to try out? Try out creating a bullet point summary, action items from support or from the customer side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<Feel free to come up with your own prompt!>\n",
    "\n",
    "<ADD SUPPORT DIALOGUE HERE>\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlOgWzmNJKCT"
   },
   "source": [
    "## Section 4.  Hashtag tokenization\n",
    "Hashtag tokenization is the process of taking a piece of text and getting the hashtag \"tokens\" out of it. You can use this, for example, if you want to generate hashtags for your social media campaigns. In this example, you take [this tweet from Google Cloud](https://twitter.com/googlecloud/status/1649127992348606469) and generate some hashtags you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://img.freepik.com/free-vector/microblogging-platform-user-social-media-communication-bloggers-tool-sharing-short-messages-microblogger-post-sharing-commenting-discussion_335657-2389.jpg?w=1060&t=st=1693335487~exp=1693336087~hmac=cbf0aec9fd3719b66d60cc65355fe17e60a193c7e37a6ccdedb21c9f6aace321)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Google Cloud Tweet\n",
    "\n",
    "Since 2007, Google has been carbon neutral, matching annual electricity consumption with 100% renewable energy.\n",
    "\n",
    "Today, we’re raising the bar once again—setting a goal to run our business entirely on 24/7 carbon-free energy by 2030 ➡️ http://goo.gle/32qHpEJ\n",
    "\n",
    "Click to expand ⬇️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BWa8rNV0JKCT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#EarthDay\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Tokenize the hashtags of this tweet:\n",
    "\n",
    "<ADD TWEET CONTENT HERE>\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.8, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f-w7mUxJKCT"
   },
   "source": [
    "## Section 5. Title & heading generation\n",
    "Below, you ask the model to generate five options for possible title/heading combos for a given piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](https://img.freepik.com/free-vector/creative-writing-concept-illustration_114360-8147.jpg?w=1800&t=st=1693335891~exp=1693336491~hmac=13e54b2e9c8fa295cb514e3833092c5b84878dcb4322fc41ad3697e2b1a7e93d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWNri4DTJKCU"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a title for this text, give me five options:\n",
    "\n",
    "<ADD CONTENTS>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt, temperature=0.8, max_output_tokens=256, top_k=1, top_p=0.8\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcpmZnwKJKCU"
   },
   "source": [
    "## Section 6. Evaluation\n",
    "You can evaluate the outputs from summarization tasks using [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) as an evalulation framework. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n",
    "\n",
    "\n",
    "The first step is to install the ROUGE library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJcl38ElJKCU"
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD9eKq3SJKCU"
   },
   "source": [
    "Create a summary from a language model that you can use to compare against a human-generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37m_fb-HJKCU"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ROUGE = Rouge()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Provide a very short, maximum four sentences, summary for the following article:\n",
    "\n",
    "Our quantum computers work by manipulating qubits in an orchestrated fashion that we call quantum algorithms.\n",
    "The challenge is that qubits are so sensitive that even stray light can cause calculation errors — and the problem worsens as quantum computers grow.\n",
    "This has significant consequences, since the best quantum algorithms that we know for running useful applications require the error rates of our qubits to be far lower than we have today.\n",
    "To bridge this gap, we will need quantum error correction.\n",
    "Quantum error correction protects information by encoding it across multiple physical qubits to form a “logical qubit,” and is believed to be the only way to produce a large-scale quantum computer with error rates low enough for useful calculations.\n",
    "Instead of computing on the individual qubits themselves, we will then compute on logical qubits. By encoding larger numbers of physical qubits on our quantum processor into one logical qubit, we hope to reduce the error rates to enable useful quantum algorithms.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "candidate = generation_model.predict(\n",
    "    prompt, temperature=0.1, max_output_tokens=1024, top_k=40, top_p=0.9\n",
    ").text\n",
    "\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b44f9872e1ba"
   },
   "source": [
    "You will also need a human-generated summary that we will use to compare to the `candidate` generated by the model. We will call this `reference`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0qNdPzOJKCc"
   },
   "outputs": [],
   "source": [
    "reference = \"Quantum computers are sensitive to noise and errors. To bridge this gap, we will need quantum error correction. Quantum error correction protects information by encoding across multiple physical qubits to form a “logical qubit”.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KKaYhzwJKCc"
   },
   "source": [
    "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
    "\n",
    "- `rouge-1`, which measures unigram overlap\n",
    "- `rouge-2`, which measures bigram overlap\n",
    "- `rouge-l`, which measures the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHUH6VuTJKCc"
   },
   "outputs": [],
   "source": [
    "ROUGE.get_scores(candidate, reference)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_summarization.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
